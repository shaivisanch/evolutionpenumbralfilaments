{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2059f8d9-fa20-41fc-ac20-119d380eecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6779f-5694-4c4c-829d-32fba7d1120d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@2.610] global /Users/runner/miniforge3/conda-bld/libopencv_1666820195419/work/opencv_contrib/modules/xfeatures2d/misc/python/shadow_sift.hpp (15) SIFT_create DEPRECATED: cv.xfeatures2d.SIFT_create() is deprecated due SIFT tranfer to the main repository. https://github.com/opencv/opencv/issues/16736\n"
     ]
    }
   ],
   "source": [
    "img1 = cv2.imread(\"/sanhome/shaivi/data/Phil_Shirts/penumbra_00.png\")\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "img2 = cv2.imread(\"/sanhome/shaivi/data/Phil_Shirts/penumbra_01.png\")\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a new feature detector object\n",
    "feature_detector = cv2.FastFeatureDetector_create()\n",
    "\n",
    "# Detect features in the first image\n",
    "keypoints1 = feature_detector.detect(gray1)\n",
    "\n",
    "# Create a new feature descriptor object\n",
    "descriptor_extractor = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# Compute the feature descriptor for the first image\n",
    "keypoints1, descriptor1 = descriptor_extractor.compute(gray1, keypoints1)\n",
    "\n",
    "# Detect features in the second image\n",
    "keypoints2 = feature_detector.detect(gray2)\n",
    "\n",
    "# Compute the feature descriptor for the second image\n",
    "keypoints2, descriptor2 = descriptor_extractor.compute(gray2, keypoints2)\n",
    "\n",
    "# Match the features between the two images using the descriptor\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck = True)\n",
    "matches = bf.match(descriptor1, descriptor2)\n",
    "\n",
    "# Draw the matches on the images\n",
    "img_matches = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches, None)\n",
    "\n",
    "# Show the matched images\n",
    "cv2.imshow(\"Matched Features\", img_matches)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af0dc2-6995-4105-af91-be98822d2de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##need to figure out why its showing weird colors... maybe its the drawMatches part\n",
    "## need to figure out how to do this wiht less sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423218c8-954a-41c8-a55f-2a3b2302b828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72871ea8-0f28-4610-8bb5-80bf39197cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6d284-c7c6-4b8f-b571-1e89f52fb18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e3f94-1a39-4d3d-82fe-b2790bdbe144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e048c980-f7ae-4d57-93f9-b358788a6255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error opening video file\n",
      "Error reading frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"video.mp4\"\n",
      "[ERROR:0@4.103] global /Users/runner/miniforge3/conda-bld/libopencv_1666820195419/work/modules/videoio/src/cap.cpp (166) open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.6.0) /Users/runner/miniforge3/conda-bld/libopencv_1666820195419/work/modules/core/src/utils/filesystem.cpp:580: error: (-213:The function/feature is not implemented)  in function 'exists'\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /Users/runner/miniforge3/conda-bld/libopencv_1666820195419/work/modules/highgui/src/window.cpp:967: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading frame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Select the region of interest (ROI) for feature tracking\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m roi \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectROI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create a tracker object\u001b[39;00m\n\u001b[1;32m     21\u001b[0m tracker \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mTrackerKCF_create()\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /Users/runner/miniforge3/conda-bld/libopencv_1666820195419/work/modules/highgui/src/window.cpp:967: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Create a VideoCapture object and read in the video file\n",
    "cap = cv2.VideoCapture('file:///sanhome/shaivi/data/Phil_Shirts/SPcubes_20170915_224806_magfield_index.fits')\n",
    "\n",
    "# Check if the video was opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "\n",
    "# Read the first frame of the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Check if the frame was read successfully\n",
    "if not ret:\n",
    "    print(\"Error reading frame\")\n",
    "\n",
    "# Select the region of interest (ROI) for feature tracking\n",
    "roi = cv2.selectROI(frame)\n",
    "\n",
    "# Create a tracker object\n",
    "tracker = cv2.TrackerKCF_create()\n",
    "\n",
    "# Initialize the tracker with the ROI and first frame\n",
    "tracker.init(frame, roi)\n",
    "\n",
    "# Loop through the video frames\n",
    "while True:\n",
    "    # Read the next frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the frame was read successfully\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Update the tracker and get the updated ROI\n",
    "    success, roi = tracker.update(frame)\n",
    "\n",
    "    # Check if the tracker was able to update the ROI\n",
    "    if success:\n",
    "        # Draw a rectangle around the ROI\n",
    "        (x, y, w, h) = tuple(map(int, roi))\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Check if the user pressed 'q' to quit\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbec8c7-79e7-4d45-8637-1c4a96533e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
